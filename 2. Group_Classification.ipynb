{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Group Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having understood the best subdivision of scenarios into groups by taking the *six most \"distant\" clusters in the multidimensional space* (in Scenario-Grouping.ipynb), we are ready to train the model to make it classify **questions** into **groups**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having imported the needed libraries we load our train dataframe and the [**spaCy** model](https://spacy.io/models/en#en_core_web_lg) we will use.\n",
    "We use the *large model* because we will need vectors for *word embedding*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "train_df = pd.read_csv('dataset_intent_train.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new column \"group\", that is going to be our label, by clustering scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def grouping(df):\n",
    "    groups = []\n",
    "    for i in df['scenario']:\n",
    "        if i in ['weather', 'cooking', 'transport', 'general', 'social',\n",
    "                    'news', 'takeaway', 'qa']:\n",
    "            groups.append('a')\n",
    "        elif i in ['music', 'audio', 'play']:\n",
    "            groups.append('b')\n",
    "        elif i in ['recommendation', 'lists', 'datetime', 'calendar']:\n",
    "            groups.append('c')\n",
    "        elif i == 'alarm':\n",
    "            groups.append('d')\n",
    "        elif i == 'iot':\n",
    "            groups.append('e')\n",
    "        elif i == 'email':\n",
    "            groups.append('f')\n",
    "            \n",
    "    df['group'] = groups\n",
    "    return df        \n",
    "            \n",
    "grouping(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then vectorize the questions, creating a *300 dimensions word embedding*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['vector'] = [nlp(text).vector for text in train_df.question]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our **X** and __y__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df[['question', 'vector']]\n",
    "y = train_df['group']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want our \"vector\" column to be a Series of length 300, but rather to add 300 new columns (**features**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in X.iterrows():\n",
    "    for j, vec in enumerate(X.loc[i, 'vector']):\n",
    "        X.loc[i, f'Vec_{j+1}'] = vec\n",
    "X = X.drop('vector', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our:\n",
    "> * Term Frequency - Inverse Document Frequency analyzer: proceding \"hunder the hood\" through a Bag-of-Words\n",
    "> - Preprocessor: tfidf on question and normalizing the question-vector dimensions\n",
    "> * Classifier: Linear Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1, 2))\n",
    "preproc = ColumnTransformer([('tfidf', tfidf, 'question'),\n",
    "                             ('scaler', Normalizer(), [i for i in X.columns[1:]])])\n",
    "lsvc = LinearSVC(C=1.7, loss='hinge', max_iter=10000, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now check our accuracy cross-validating via 10 different train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.959550561797753\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    pipe = make_pipeline(preproc, lsvc).fit(X_train, y_train)\n",
    "    pred = pipe.predict(X_test)\n",
    "    acc.append(accuracy_score(y_test, pred))\n",
    "print(np.array(acc).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit our entire train dataframe to our Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_t = make_pipeline(preproc, lsvc).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the test dataframe and repeat the previous vectorization processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('testset_notarget.csv').drop('Unnamed: 0', axis=1)\n",
    "df_test['vector'] = [nlp(text).vector for text in df_test.question]\n",
    "Xt = df_test[['question', 'vector']]\n",
    "\n",
    "for i, row in Xt.iterrows():\n",
    "    for j, vec in enumerate(Xt.loc[i, 'vector']):\n",
    "        Xt.loc[i, f'Vec_{j+1}'] = vec\n",
    "Xt = Xt.drop('vector', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, finally, we predict the test questions groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_t = pipe_t.predict(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>pred_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>delete item on list</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what brand hair spray does donald trump use</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>play the song by michael jackson</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what events are near me</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>can you reserve a ticket to grand rapids by train</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question pred_group\n",
       "0                                delete item on list          c\n",
       "1        what brand hair spray does donald trump use          a\n",
       "2                   play the song by michael jackson          b\n",
       "3                            what events are near me          c\n",
       "4  can you reserve a ticket to grand rapids by train          a"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out = pd.concat([df_test, pd.Series(pred_t)], axis=1).drop('vector', axis=1).rename({0: 'pred_group'}, axis=1)\n",
    "df_out.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are now ready to proceed to the intent classifcation through BERT."
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
